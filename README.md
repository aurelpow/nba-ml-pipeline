# NBA Player Predictions - End to End Pipeline (Docker +Google Cloud )

*A complete, modular pipeline for fetching, processing, modeling, and predicting NBA player performance. 
Whether youâ€™re exploring the data in a notebook or running daily inference in production, this repo has you covered.*

I built this because I love **basketball + dataðŸ€ðŸ“ˆ**. 

---

## ðŸ“¥ Data Sources and Ingestion
[swar/nba_api](https://github.com/swar/nba_api/)

   - **Players**: Retrieve active rosters via the NBA Stats API
      - Source : [swar/nba_api/stats/endpoints/playerindex](https://github.com/swar/nba_api/blob/master/src/nba_api/stats/endpoints/playerindex.py)
      - Ingestion : [src/get_nba_players](src/get_nba_players.py)
   - **Teams**: Retrieve team metadata via the NBA Stats API.
      - Source : [swar/nba_api/stats/static/teams](https://github.com/swar/nba_api/blob/master/src/nba_api/stats/static/teams.py)
      - Ingestion : [src/get_nba_teams](src/get_nba_teams.py)
   - **Boxscores**: Pull both basic and advanced boxscore statistics for every game.
      - **Basic Boxscore**: 
         - Source : [swar/nba_api/stats/endpoints/boxscoretraditionalv3](https://github.com/swar/nba_api/blob/master/src/nba_api/stats/endpoints/boxscoretraditionalv3.py)
         - Ingestion : [src/get_nba_boxscore_basic](src/get_nba_boxscore_basic.py) 
      - **Advanced Boxscore**
         - Source : [swar/nba_api/stats/endpoints/boxscoreadvancedv3](https://github.com/swar/nba_api/blob/master/src/nba_api/stats/endpoints/boxscoreadvancedv3.py)
         - Ingestion : [src/get_nba_advanced_boxscore](src/get_nba_advanced_boxscore.py) 
   - **Schedule**: Fetch all game schedules for a specific season.
      - Source [swar/nba_api/stats/endpoints/scheduleleaguev2](https://github.com/swar/nba_api/blob/master/src/nba_api/stats/endpoints/scheduleleaguev2.py)
      - Ingestion : [src/get_nba_schedule.py](src/get_nba_schedule.py)

> ðŸ” NBA API calls can use a private proxy ([DecoDO](https://dashboard.decodo.com/welcome)) via `HTTP_PROXY` / `HTTPS_PROXY`. â€” avoids timeouts  
> In Cloud Run, mount these from **Secret Manager**.

## ðŸ§  Machine Learning Model

- **Notebook**:  [NBA_Players_Points_Prediction_ML](ml_dev/notebooks/NBA_Players_Points_Prediction_ML.ipynb)
  - Data exploration & cleaning
  - Feature engineering (touches, shooting splits, contested/uncontested, defended-at-rim, opponent/position effects, rolling windows)
  - Model selection: **LightGBM** for Points (PTS)
  - Evaluation & tuning (metrics + plots)
  - Export artifact: `best_lgbm_model.pkl`
### ðŸ§­ How to Train & Export
1. Open the notebook `ml_dev/notebooks/NBA_Players_Points_Prediction_ML.ipynb`
2. Run training cells â†’ evaluate â†’ persist the **best** model:
   - Local:
     ```python
     joblib.dump(model, "ml_dev/models/best_lgbm_model.pkl")
     ```
   - GCS: Copy and paste the model to a google cloud bucket

---
## ðŸ§° Data Prep & Inference
**Goal**: prepare the inputs to the exact feature schema the trained model expects, then generate player-game predictions.
Core utilities live in `common/`:
- `parser.py`, `utils.py`, `io_utils.py`, `constants.py`
- Tasks: schema normalization, joins (players/teams â†” boxscores), type casting, dedup, and quality checks.

### Inference ([src/get_predictions_stats_points.py](src/get_predictions_stats_points.py))
1) **Load schedule** for `DATE â€¦ DATE + DAYS_NUMBER` (`get_nba_schedule.py`).
2) Expand to **player-game** rows for active rosters.
3) **Load model** from `MODEL_PATH` (local path or `gs://â€¦`):
   the loader downloads from GCS at runtime if needed.
4) Build the **same feature set** used at train time for each player-game.
5) **Predict** points (PTS). Optionally compute fantasy/scoring aggregates.
6) **Persist (by `SAVE_MODE`)**
   - `local` â†’ `predictions_${DATE}.csv`
   - `bq`    â†’ BigQuery table (configured in `io_utils.py` / `constants.py`)

## ðŸ“ Repository Structure

```
NBA_project_ML/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ run_all.sh            # Orchestrates all processes via env-vars
â”œâ”€â”€ requirements.txt      # Core Python dependencies
â”œâ”€â”€ main.py               # CLI entrypoint for individual subprocesses
â”œâ”€â”€ src/                  # Modular ETL and inference scripts
â”‚   â”œâ”€â”€ get_nba_players.py
â”‚   â”œâ”€â”€ get_nba_teams.py
â”‚   â”œâ”€â”€ get_nba_boxscore_basic.py
â”‚   â”œâ”€â”€ get_nba_advanced_boxscore.py
â”‚   â”œâ”€â”€ get_nba_schedule.py
â”‚   â””â”€â”€ get_predictions_stats_points.py
â”œâ”€â”€ common/               # Shared utilities, parsers, and singletons
â”‚   â”œâ”€â”€ common.py
â”‚   â”œâ”€â”€ io_utils.py
â”‚   â”œâ”€â”€ parser.py
â”‚   â”œâ”€â”€ singleton_meta.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ ml_dev/
â”‚   â”œâ”€â”€ notebooks/        # Jupyter notebooks for EDA & model development
â”‚   â”‚   â””â”€â”€ NBA_Players_Points_Prediction_ML.ipynb
â”‚   â””â”€â”€ models/           # Serialized model artifacts
â”‚       â””â”€â”€ best_lgbm_model_v2.pkl
â”œâ”€â”€ databases/            # Raw and processed data files
â”‚   â”œâ”€â”€ nba_boxscore_basic.csv
â”‚   â”œâ”€â”€ nba_boxscore_advanced.csv
â”‚   â”œâ”€â”€ nba_future_games_df.csv
â”‚   â”œâ”€â”€ nba_players_df.csv
â”‚   â”œâ”€â”€ nba_points_predictions_df.csv
â”‚   â””â”€â”€ nba_teams_df.csv
â””â”€â”€ README.md             # You are here
```

---
## âš™ï¸ Configuration (env vars)

| Var | Required | Example | Notes |
|---|---|---|---|
| `SEASON` | âœ… | `2024-25` | Target season |
| `SEASON_TYPE` | â• | `Regular Season` | Default: Regular Season |
| `DATE` | âœ… | `2025-05-01` | Start date for inference |
| `DAYS_NUMBER` | â• | `1` | Days ahead |
| `SAVE_MODE` | â• | `local` \| `bq` | CSV vs BigQuery |
| `MODEL_PATH` | â• | `ml_dev/models/best_lgbm_model.pkl` \| `gs://â€¦/best_lgbm_model.pkl` | Local or GCS |
| `HTTP_PROXY` / `HTTPS_PROXY` | â• | secret | Use in cloud to avoid API timeouts |

> If `MODEL_PATH` starts with `gs://`, the app downloads the file at runtime (see `common/io_utils.py::load_model()`).

## âš™ï¸  Setup

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-org/NBA_project_ML.git
   cd NBA_project_ML
   ```

2. **Build the Docker image**

   ```bash
   docker build -t nba-pipeline:latest .
   ```

3. **(Optional) Local Python environment**

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

---

## ðŸ”„ Running the Pipeline

### A) Local (CSV)
To run a specific process 
```bash
python -u main.py -p get_predictions_stats_points -s 2024-25 -d "2025-04-13" -m "ml_dev/models/best_lgbm_model.pkl" -sm "local"
# -> ./databases/nba_points_predictions_df.csv
```
>-p process, -s season, -d date, -m model path, -sm save mode.

### B) Docker
```bash
docker run --rm \
  -e SEASON="2024-25" -e SEASON_TYPE="Regular Season" \
  -e DATE="2025-05-01" \
  -e SAVE_MODE="local" \
  -e MODEL_PATH="ml_dev/models/best_lgbm_model.pkl" \
  nba_project_ml:latest
```
### C) Cloud Run Job (BigQuery + proxy secret)
```bash
# build & push (see cloudbuild.yaml) or:
PROJECT_ID="your-gcp-project"
REGION="us-central1"
ARTIFACT_REPO="nba-docker-repo"
IMAGE_NAME="nba_project"
IMAGE_URI="${REGION}-docker.pkg.dev/${PROJECT_ID}/${ARTIFACT_REPO}/${IMAGE_NAME}:latest"

BUCKET_MODELS="gs://your-bucket/models_trained"
MODEL_PATH="${BUCKET_MODELS}/best_lgbm_model.pkl"

# one-time proxy secret (DecoDO URL)
# gcloud secrets create PROXY_URL --data-file=<(echo -n "http://user:pass@host:port")

# Create or update the Cloud Run Job with env vars + secrets
gcloud run jobs create nba-prediction-job \
  --image "$IMAGE_URI" \
  --region "$REGION" \
  --set-env-vars=SEASON=2024-25,SEASON_TYPE="Regular Season",DATE=2025-05-01,DAYS_NUMBER=1,SAVE_MODE=bq,MODEL_PATH="${MODEL_PATH}" \
  --set-secrets=HTTPS_PROXY=PROXY_URL:latest,HTTP_PROXY=PROXY_URL:latest \
  --max-retries=1 --memory=1Gi --cpu=1 --task-timeout=1800s \
|| gcloud run jobs update nba-prediction-job \
  --image "$IMAGE_URI" \
  --region "$REGION" \
  --set-env-vars=SEASON=2024-25,SEASON_TYPE="Regular Season",DATE=2025-05-01,DAYS_NUMBER=1,SAVE_MODE=bq,MODEL_PATH="${MODEL_PATH}" \
  --set-secrets=HTTPS_PROXY=PROXY_URL:latest,HTTP_PROXY=PROXY_URL:latest

# Execute ad-hoc
gcloud run jobs execute nba-prediction-job --region "$REGION"
```
## ðŸ—ºï¸ Modes & Outputs

- Run: local ðŸ–¥ï¸ / docker ðŸ³ / cloud â˜ï¸
- Save: SAVE_MODE=local â†’ ðŸ“„ CSV | SAVE_MODE=bq â†’ ðŸ—„ï¸ BigQuery

## ðŸ“„ License & Credits

- **Author**: Aurelien Pow ([@aurelpow](https://github.com/aurelpow))

## ðŸ›£ï¸ Next Improvements and Features
- **ðŸ” Automated Retraining**: Add a scheduled job to retrain the model weekly or monthly when performance degrades.
- **âž• More stats** (AST / TOV / REB)
- **ðŸ©º Injury-aware predictions**
- **ðŸŒ API Service**: Expose predictions via a REST API (FastAPI/Flask) for real-time applications.
- **ðŸ“Š Dashboard**: Build an interactive dashboard (Plotly Dash or Power BI) to visualize predictions and model performance.